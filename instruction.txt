我的前端（React Native）可以接受麦克风输入的音频文件或文本框输入的文本。（当前前端缺乏文本框，你需要在手机视图里做一下）
这些输入会发送给我的Spring Boot后端，后端再调用一个Python的NLP服务（当前名为`voice_module`，希望重构为`nlp_service`）。

`nlp_service`的核心功能包括：
1.  语音转文字 (STT)：可选，仅当输入为音频时执行。
2.  自然语言理解 (NLU)：核心功能，对STT结果或直接输入的文本进行处理，提取意图（如操作动作、家居实体）。
3.  文字转语音 (TTS)：可选，根据用户设置决定是否对NLU生成的响应进行语音播报。

我当前 `voice_module` 的目录结构包括：`config`(包含yaml文件), `core`（我原本用来组织3个功能的地方）, `data`, `device_mapping`, `interfaces`(存放`stt_interface`, `nlp_interface`, `tts_interface`这些抽象接口), `nlp`(即NLU模块，下面有`processors`子目录), `speech_to_text` (包含`engines`目录和`factory.py`), `text_to_speech` (包含`engines`目录和`factory.py`), 以及 `tests`, `utils`。

我希望你协助我将`voice_module`重构为`nlp_service`，使其成为一个通过API（推荐FastAPI）被后端调用的服务。同时，请为NLU功能提供一个临时的填充实现。我还希望识别出的文本和NLU结果能在前端的提示框中显示。请指导我如何组织`requirements.txt`，并提供关于Spring Boot后端如何与此Python服务交互以及如何构造响应给React Native前端的建议和代码片段。

---
**第一部分：Python `nlp_service` 重构与FastAPI服务封装**
---

*基于我现有的Python `voice_module`结构，请将其重构为一个名为 `nlp_service` 的新Python项目，并使用FastAPI框架将其封装成一个API服务。”

1.  **项目设置和结构调整：**
    * “创建一个名为 `nlp_service` 的根目录。”
    * “将我现有 `voice_module` 的所有子目录（`config`, `core`, `data`, `device_mapping`, `interfaces`, `nlp`, `speech_to_text`, `text_to_speech`, `tests`, `utils`）移动或复制到这个新的 `nlp_service` 根目录下。”（记得把我原有的engines迁移过来）
    * “在 `nlp_service` 根目录内，创建一个名为 `app` 的子目录，用于存放主要的FastAPI应用和核心服务逻辑。”
    * “为了提高代码清晰度，请将原有的 `speech_to_text` 文件夹重命名为 `stt`，将 `text_to_speech` 文件夹重命名为 `tts`，并将原有的 `nlp` 文件夹（主要执行NLU功能）保持或重命名为 `nlu`。请在所有代码中同步更新相关的导入路径。”

2.  **核心编排器类 (`app/orchestrator.py`)：**
    * “在 `app` 目录下创建 `orchestrator.py` 文件，并定义一个名为 `NLPServiceOrchestrator` 的类。”
    * “`NLPServiceOrchestrator` 类的 `__init__` 方法应负责从 `../config` 目录加载配置，并使用我项目中已有的工厂模式（例如 `stt/factory.py`, `nlu/factory.py`, `tts/factory.py`）和接口定义（在 `interfaces` 目录下）来初始化STT、NLU、TTS引擎的实例。”
    * “在 `NLPServiceOrchestrator` 类中实现以下私有的异步辅助方法：”
        * `async def _perform_stt(self, audio_data: bytes) -> str:` （调用STT引擎实例将音频字节流转换为文本）
        * `async def _perform_nlu(self, text: str) -> dict:` （调用NLU引擎实例处理文本，具体占位逻辑见后续指令）
        * `async def _perform_tts(self, text_to_speak: str) -> Union[str, bytes, None]:` （调用TTS引擎实例，返回TTS音频的URL、Base64编码的字节数据，或None。具体返回哪种形式，请先提供一个占位，例如返回一个文件名字符串。）
    * “在 `NLPServiceOrchestrator` 类中实现以下两个主要的公开异步处理方法，供FastAPI端点调用：”
        * `async def handle_audio_input(self, audio_data: bytes, settings: dict) -> dict:`
            * “此方法首先调用 `_perform_stt` 将 `audio_data` 转换为 `transcribed_text`。”
            * “然后，调用 `_perform_nlu` 处理 `transcribed_text`。”
            * “检查 `settings` 字典中的 `tts_enabled` 键 (例如, `settings.get('tts_enabled', True)`，如果未提供则默认为 `True`)。如果为 `True`，则根据NLU结果生成合适的响应文本并调用 `_perform_tts`。”
            * “返回一个包含处理结果的字典，其结构需符合后文‘API响应设计（Python端）’部分的描述。”
        * `async def handle_text_input(self, text_input: str, settings: dict) -> dict:`
            * “此方法直接调用 `_perform_nlu` 处理 `text_input` (跳过STT步骤)。”
            * “与 `handle_audio_input` 类似，根据 `settings` 和NLU结果条件性调用 `_perform_tts`。”
            * “返回一个包含处理结果的字典，其结构需符合后文‘API响应设计（Python端）’部分的描述。”

3.  **NLU模块占位逻辑实现 (例如在 `nlu/processors/placeholder_processor.py`)：**
    * “在NLU模块中（例如，在一个实现了我的 `NLUInterface` 接口的新文件 `nlu/processors/placeholder_processor.py` 中），创建如下临时的NLU处理逻辑：”
        * `async def understand(self, text: str) -> dict:`
            * “如果 `text` 包含‘开’和‘灯’，则返回 `{'action': 'TURN_ON', 'entity': 'light', 'location': '客厅'}`。”
            * “如果 `text` 包含‘关’和‘空调’，则返回 `{'action': 'TURN_OFF', 'entity': 'air_conditioner'}`。”
            * “其他情况下，返回 `{'action': 'UNKNOWN', 'entity': None}`。”
    * “确保我的NLU工厂类（`nlu/factory.py`）能够创建并使用这个占位处理器的实例。”

4.  **FastAPI 应用 (`app/main.py`):**
    * “创建 `app/main.py` 文件。在其中初始化一个FastAPI应用实例，并实例化 `NLPServiceOrchestrator`。”
    * “定义两个不同的POST端点：”
        * `@app.post("/process_audio")`
            * “此端点应接收上传的音频文件（使用 `audio_file: UploadFile = File(...)`）和可选的TTS设置（例如，可以作为一个JSON字符串通过 `settings_json: str = Form("{}")` 接收，然后在代码中解析，或者通过查询参数 `tts_enabled: bool = True`）。`python-multipart` 库是处理文件上传所必需的。”
            * “端点逻辑将读取音频文件为字节流，解析设置，然后调用 `orchestrator_instance.handle_audio_input(audio_bytes, parsed_settings)`。”
            * “返回编排器的处理结果。”
        * `@app.post("/process_text")`
            * “此端点应接收一个JSON负载。请为此定义一个Pydantic模型，例如 `class TextCommandPayload(BaseModel): text_input: str; settings: Optional[dict] = {}`。”
            * “端点逻辑将调用 `orchestrator_instance.handle_text_input(payload.text_input, payload.settings)`。”
            * “返回编排器的处理结果。”
    * “请在端点处理器中实现基础的try-except错误处理逻辑，以便在发生错误时返回包含错误信息的JSON和适当的HTTP状态码。”

---
**第二部分：`requirements.txt` (Python `nlp_service`)**
---

**给Cursor的指令：** “请在 `nlp_service` 项目的根目录下创建或更新 `requirements.txt` 文件。”
* “扫描 `nlp_service` 目录下的所有Python文件中的导入语句，并列出所有必需的库。关键库包括 `fastapi`, `uvicorn[standard]`, `python-multipart`。此外，还需包含我的STT引擎（如 `torch`, `torchaudio`, `transformers`, `soundfile`, 以及任何特定的STT库如 `openai-whisper` 或 `realtimeSTT`）、NLU模块（如果依赖特定库）和TTS引擎所需的库。”
* “请先列出库名。如果我知道某些库的确切版本，我会后续提供。”

---
**第三部分：API响应设计 (Python `nlp_service` 端)**
---

**给Cursor的指令：** “Python FastAPI服务的 `/process_audio` 和 `/process_text` 端点，都应返回如下结构的JSON响应。此响应将由Spring Boot后端接收。”

**Python服务响应JSON结构示例:**
```json
{
    "input_type": "audio", // 或 "text"
    "transcribed_text": "打开客厅的灯", // 仅当 input_type 为 "audio" 时有意义，否则为 null 或原始输入文本
    "nlu_result": {
        "action": "TURN_ON",
        "entity": "light",
        "location": "客厅"
        // 可以根据NLU占位逻辑的实际输出来调整这里的字段
    },
    "response_message_for_tts": "好的，正在为您打开客厅的灯。", // 基于NLU结果生成的、供TTS朗读的文本
    "tts_output_reference": "temp_audio/response_123.wav", // TTS生成的音频文件的相对路径或标识符，或 null
    "status": "success", // 或 "error"
    "error_message": null // 仅当 status 为 "error" 时有内容
}
“请确保 NLPServiceOrchestrator 的方法产生符合此结构的字典。”
“tts_output_reference 可以是一个临时文件名或一个可以用于后续获取音频数据的标识。如果TTS禁用或失败，则为 null。”
第四部分：Spring Boot 后端交互逻辑 (给Cursor的指令以生成Java代码建议)
给Cursor的指令： “现在，请为我的Spring Boot后端提供与上述Python nlp_service 交互的建议和Java代码片段。”

配置：

“在Spring Boot的 application.properties (或 .yml) 文件中，建议添加Python NLP服务的URL配置项，例如 nlp.service.baseurl=http://localhost:8000 (假设Python服务运行在8000端口)。”
创建Java Controller (例如 VoiceCommandController.java)：

“创建一个新的Spring MVC Controller，例如 VoiceCommandController，并映射一个基础路径如 /api/command。”
“在此Controller中创建两个POST端点：”
public ResponseEntity<?> handleAudioCommand(@RequestParam("audio") MultipartFile audioFile, @RequestParam(value = "settings", required = false, defaultValue = "{}") String settingsJson) 映射到 /api/command/audio:
“此方法接收React Native前端上传的 audioFile (类型为 MultipartFile) 和一个可选的 settingsJson 字符串 (包含如 {\"tts_enabled\": true} 的设置)。”
“在此方法内部，需要使用 RestTemplate 或 WebClient 来调用Python服务的 /process_audio 端点。”
“构造一个 MultiValueMap<String, Object> 作为请求体，将音频文件（audioFile.getResource()）和设置（settingsJson）添加到map中。”
“设置请求头为 HttpHeaders.CONTENT_TYPE, MediaType.MULTIPART_FORM_DATA_VALUE。”
“发送POST请求到 nlp.service.baseurl + /process_audio。”
“接收Python服务的JSON响应，并将其（可能经过一些转换）作为 ResponseEntity 返回给React Native前端。请为这个响应设计一个Java DTO类，结构参考下一节‘前端交互与数据显示’。”
public ResponseEntity<?> handleTextCommand(@RequestBody TextCommandPayload payload) 映射到 /api/command/text:
“为此方法创建一个Java DTO类 TextCommandPayload，包含 String textInput 和 Map<String, Object> settings 字段，以匹配前端发送的JSON。”
“使用 RestTemplate 或 WebClient 调用Python服务的 /process_text 端点。”
“将 TextCommandPayload 对象序列化为JSON字符串作为请求体。”
“设置请求头为 HttpHeaders.CONTENT_TYPE, MediaType.APPLICATION_JSON_VALUE。”
“发送POST请求到 nlp.service.baseurl + /process_text。”
“接收Python服务的JSON响应，并将其（可能经过一些转换）作为 ResponseEntity 返回给React Native前端。复用或创建新的Java DTO响应类。”
Java DTOs (数据传输对象)：

“请为Spring Boot后端定义用于接收Python服务响应和发送给React Native前端响应的Java DTO类。例如：”
Java

// DTO for response from Python service (simplified)
// public class NlpServiceResponse {
//     private String inputType;
//     private String transcribedText;
//     private NluResultDto nluResult;
//     private String responseMessageForTts;
//     private String ttsOutputReference;
//     private String status;
//     private String errorMessage;
//     // Getters and Setters
// }
// public class NluResultDto {
//     private String action;
//     private String entity;
//     private String location;
//     // Getters and Setters
// }

// DTO for response TO React Native frontend
public class FrontendResponseDto {
    private String sttText; // STT结果，如果是语音输入
    private NluResultDto nluResult; // NLU结果
    // private String ttsAudioUrl; // 如果需要通过Spring Boot代理TTS音频，则提供一个URL
    private boolean success;
    private String errorMessage;
    // Getters and Setters
}
“请根据Python服务的实际响应结构来完善这些DTO。”
第五部分：前端交互与数据显示 (指导Spring Boot响应以支持前端)
给Cursor的指令： “为了让我的React Native前端能够在一个提示框中显示STT识别文本和NLU结果，请确保Spring Boot后端在调用Python nlp_service 后，返回给前端的JSON响应中包含以下关键信息。请为Spring Boot的Controller方法生成这样的响应结构。”

Spring Boot响应给React Native的JSON结构示例:

JSON

{
    "sttText": "打开客厅的灯", // 如果是语音输入，则为STT识别的文本；如果是文本输入，则为null或原始输入文本。
    "nluResult": {             // NLU处理结果
        "action": "TURN_ON",
        "entity": "light",
        "location": "客厅"
        // 可根据需要添加NLU返回的其他字段
    },
    // "ttsAudioUrl": "http://<your-spring-boot-ip>:<port>/api/audio/tts_response_123.wav", // 可选：如果TTS音频由Spring Boot提供下载
    "success": true, // 操作是否成功
    "errorMessage": null // 如果操作失败，则包含错误信息
}
“Spring Boot Controller在收到Python服务的响应后，应从中提取相关信息（如transcribed_text、nlu_result），并组装成上述FrontendResponseDto这样的结构返回给React Native客户端。”
“如果Python服务返回了 tts_output_reference，Spring Boot可以选择：（1）将此引用（可能是文件名）转换为一个可供前端访问的URL，Spring Boot需要额外实现一个端点来根据此引用提供音频文件；（2_）直接将音频数据（如果Python服务返回的是Base64）透传给前端；（3）如果前端不需要语音播报或TTS被禁用，则此部分可以省略。”
总结性提示给Cursor：
“请在整个过程中注重代码的模块化、清晰性和错误处理。”
“在生成Python和Java代码时，请使用相应的类型提示和注解。”
“如果需要做设计决策（例如某个DTO的具体字段，错误处理方式），请给出你的建议并解释原因。”
